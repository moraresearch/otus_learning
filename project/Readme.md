# Проектная работа. Отказоустойчивый ELK кластер. 

### Цель задания: 
Автоматизировать введение в работу кластера ELK.

### Последовательность действий:

Создать 7 нод : 
  1. 3 "сервер" ноды, на который в последствии будет настроен elasticsearch  
  2. прокси - необходим как удобно резервируемая точка отказа 
  3. бакула - бэкап сервер
  4. клиент - должен же кто то отправить данные в кластер
  5. ансибл - при проверке не у всех может быть ансибл на гипервизоре
#### vagrant up

Подключение к машине с ансиблом 
vagrant ssh ansible 

 Переход в папку с ansible инфраструктурой 
#### cd /vagrant/ansible 
 
Разом настроить вcё
#### ansible-playbook playbooks/project

### Проверка:

Для начала надо подождать минут 10 чтобы там всё устаканилось ( пока логи запишутся пока примутся на отправку пока распарсятся logstash пока утрамбуются в elastic ) 
Если всё это прошло, значит мы увидим какие либо индексы по этой ссылке: 
#### http://192.168.11.151/app/kibana#/management/elasticsearch/index_management/indices 

Теперь надо проверить работу кластера: 
#### curl -XGET http://192.168.11.142:9200/_cluster/health?pretty

данный запрос должен показать количество нод - 3 

Теперь мы должны проверить работу бэкап сервера, в принципе достаточно зайти на него и проверить историю 

#### ssh mora@192.168.11.152 -i ./id_rsa 

#### sudo su 

#### bconsole 

#### status 5 


Проверка завершена, почти отказоустойчивый кластер настроен, а значит выход из строя любой ноды не угробит работу и доступ к кластеру, но есть оговорка: proxy. Дело в том что полностью исключить точку отказа нельзя, потому что клиент(под клиентом тут понимается не только машина отправляющая логи, но и человек желающий эти логи посмотреть через веб морду) так или иначе всё равно обращается к точке(доменное имя), а не к кластеру, поэтому выделена нода при падении которой достаточно, возможно не очень свежую версию её же, поднять в другом месте и это не сильно повлияет на работу клиента (отлуп у HA минута + минута на подьем демонов).
Для примера выбран HA Proxmox и контейнер на zfs. 
В первую очередь настраивается репликация диска, чтобы на "резервном" гипервизоре были данные резервируемого контейнера, для этого на основном и резервном гипервизоре должен быть один zfs storage. 

Проверяется это так: 

#### Datacenter -> Storage 
#### В настройках выбранного stogare должны быть type - ZFS, nodes - основная и резервная. 

Далее на самом контейнере настраивается репликация 

#### В меню контейнера пункт - Replication 
#### Cоздается правило Add Target - Имя резервной ноды, Shedule частота репликации (15 минут вполне достаточно)

Репликация настроена теперь включаем переключение : 

#### Datacenter - HA - Groups 
#### Create выделяются галочками ноды, выставляются приоритеты и можно указать к примеру nofailback (не возвращать на более приоритетную ноду после её восстановления в кластере Proxmox)
#### Datacenter - HA 
#### Add выбирается резервируемый контейнер и группа созданная выше, так же рекомендую повысить Max* парамерты до 2-х, так надежнее. 

Вот теперь этот кластер может считаться отказоустойчивым. 

